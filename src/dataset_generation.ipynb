{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import qlib\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from qlib.config import REG_CN\n",
    "from qlib.contrib.data.handler import Alpha158\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 - Fetch features (alpha158) by the module `qlib`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[16904:MainThread](2023-05-23 18:18:50,728) INFO - qlib.Initialization - [config.py:416] - default_conf: client.\n",
      "[16904:MainThread](2023-05-23 18:18:51,328) INFO - qlib.Initialization - [__init__.py:74] - qlib successfully initialized based on client settings.\n",
      "[16904:MainThread](2023-05-23 18:18:51,328) INFO - qlib.Initialization - [__init__.py:76] - data_path={'__DEFAULT_FREQ': WindowsPath('D:/文件/研一下/深度学习/Quantitative Investment Based on Transformer/data/dataset_qlib')}\n",
      "[16904:MainThread](2023-05-23 18:19:37,058) INFO - qlib.timer - [log.py:128] - Time cost: 45.727s | Loading data Done\n",
      "[16904:MainThread](2023-05-23 18:19:37,298) INFO - qlib.timer - [log.py:128] - Time cost: 0.193s | DropnaLabel Done\n",
      "d:\\Download_app\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3191: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[k1] = value[k2]\n",
      "[16904:MainThread](2023-05-23 18:19:40,774) INFO - qlib.timer - [log.py:128] - Time cost: 3.476s | CSZScoreNorm Done\n",
      "[16904:MainThread](2023-05-23 18:19:40,781) INFO - qlib.timer - [log.py:128] - Time cost: 3.722s | fit & process data Done\n",
      "[16904:MainThread](2023-05-23 18:19:40,783) INFO - qlib.timer - [log.py:128] - Time cost: 49.452s | Init data Done\n"
     ]
    }
   ],
   "source": [
    "qlib.init(provider_uri=\"../data/dataset_qlib\", region=REG_CN)\n",
    "data_handler_config = {\n",
    "    \"start_time\": \"2010-01-01\",\n",
    "    \"end_time\": \"2022-12-30\",\n",
    "    \"instruments\": \"all\",\n",
    "}\n",
    "h = Alpha158(**data_handler_config)\n",
    "data_df = h.fetch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 293300 entries, 6000 to 299299\n",
      "Columns: 145 entries, date to LABEL0\n",
      "dtypes: datetime64[ns](1), float32(143), object(1)\n",
      "memory usage: 166.7+ MB\n"
     ]
    }
   ],
   "source": [
    "# Fetch the feature data\n",
    "feature_df = data_df.reset_index().dropna(axis=1, how=\"all\").rename(\n",
    "    columns={\"datetime\": \"date\", \"instrument\": \"tic\"}\n",
    ")\n",
    "# Delete features that have different NaN data among different stocks.\n",
    "drop_col = []\n",
    "for feat_name, nan_nums in feature_df.isna().sum().items():\n",
    "    if nan_nums % 100 != 0:\n",
    "        drop_col.append(feat_name)\n",
    "feature_df = feature_df.drop(columns=drop_col)\n",
    "feature_df = feature_df.dropna(how = \"any\")\n",
    "feature_df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 - Merge the features with basic price and volume  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for dataset\n",
    "alpha158 = feature_df.columns[2:].to_list() # alpha158\n",
    "basic_feature = [\"open\", \"close\", \"high\", \"low\", \"volume\"]\n",
    "target_return_span = 1\n",
    "target = f\"return+{target_return_span}\"\n",
    "time_span = 20\n",
    "CSI_date = ['20100101', '20171230', '20180101', '20191231',  '20200101', '20211231'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the basic price and volume data with features\n",
    "tic_df_list = []\n",
    "for tic_path in os.listdir(\"../data/dataset_per_stocks/\"):\n",
    "    tic_df = pd.read_csv(f\"../data/dataset_per_stocks/{tic_path}\", index_col=0)[[\"date\", \"open\", \"close\", \"high\", \"low\", \"volume\"]]\n",
    "    tic_df[\"tic\"] = tic_path[:8]\n",
    "    tic_df[target] = tic_df.close.pct_change(target_return_span).shift(-1 * target_return_span)\n",
    "    tic_df_list.append(tic_df)\n",
    "\n",
    "tic_target_df = pd.concat(tic_df_list).sort_values(by=\"date\").dropna()\n",
    "tic_target_df.date = pd.DatetimeIndex(tic_target_df.date)\n",
    "dataset_df = feature_df.merge(tic_target_df, how='inner', on=[\"date\", \"tic\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 - Generate tensor for training, evaluating and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training, valuating and testing dataset\n",
    "train_df = dataset_df[(dataset_df.date >= CSI_date[0]) & (dataset_df.date <= CSI_date[1])]\n",
    "eval_df  = dataset_df[(dataset_df.date >= CSI_date[2]) & (dataset_df.date <= CSI_date[3])]\n",
    "test_df  = dataset_df[(dataset_df.date >= CSI_date[4]) & (dataset_df.date <= CSI_date[5])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function to generate feature and label for training\n",
    "def df_2_array(dataset_df, feat_col, target, type):\n",
    "    dataset_feat = []\n",
    "    dataset_ret = []\n",
    "    dataset_price = []\n",
    "    for tic in dataset_df.tic.unique():\n",
    "        df = dataset_df[dataset_df.tic == tic]\n",
    "        feat = df[feat_col].to_numpy()\n",
    "        ret = df[target].to_numpy()\n",
    "        price = df['close'].to_numpy()\n",
    "        stock_feat = []\n",
    "        stock_ret = []\n",
    "        stock_price = []\n",
    "        for i in range(time_span, feat.shape[0]):\n",
    "            feat_standard = MinMaxScaler().fit_transform(feat[i-time_span : i]) # Standardization to (0, 1)\n",
    "            stock_feat.append(feat_standard)\n",
    "            stock_ret.append(ret[i])\n",
    "            stock_price.append(price[i])\n",
    "        stock_feat = np.array(stock_feat)\n",
    "        stock_ret = np.array(stock_ret)\n",
    "        stock_price = np.array(stock_price)\n",
    "    \n",
    "        dataset_feat.append(stock_feat)\n",
    "        dataset_ret.append(stock_ret)\n",
    "        dataset_price.append(stock_price)\n",
    "\n",
    "    dataset_feat = np.array(dataset_feat).transpose((1, 2, 0, 3))\n",
    "    dataset_ret = np.array(dataset_ret).transpose((1, 0))\n",
    "    dataset_price = np.array(dataset_price).transpose((1, 0))\n",
    "    \n",
    "    dataset_feat_tensor = torch.tensor(dataset_feat, dtype=torch.float)\n",
    "    dataset_ret_tensor = torch.tensor(dataset_ret, dtype=torch.float)\n",
    "    dataset_price_tensor = torch.tensor(dataset_price, dtype=torch.float)\n",
    "    date_list = list(map(lambda x: str(x.date()), list(sorted(set(dataset_df.date)))[time_span:]))\n",
    "\n",
    "    torch.save(dataset_feat_tensor, f\"../data/dataset_tensor/{type}/feat.pt\")\n",
    "    torch.save(dataset_ret_tensor, f\"../data/dataset_tensor/{type}/ret.pt\")\n",
    "    # torch.save(dataset_price_tensor, f\"../data/alpha/{type}/price.pt\") # The price data isn't needed yet\n",
    "    with open(f\"../data/dataset_tensor/{type}/date.txt\", \"w\") as file:\n",
    "        for date in date_list:\n",
    "            file.write(date + \"\\n\")\n",
    "    with open(f\"../data/dataset_tensor/{type}/stocks.txt\", \"w\") as file:\n",
    "        for tic in dataset_df.tic.unique():\n",
    "            file.write(tic + \"\\n\")\n",
    "    return dataset_feat, dataset_ret\n",
    "\n",
    "dataset_feat_train, dataset_ret_train = df_2_array(train_df, alpha158, target, \"train\")\n",
    "dataset_feat_eval,  dataset_ret_eval  = df_2_array(eval_df,  alpha158, target, \"eval\")\n",
    "dataset_feat_test,  dataset_ret_test  = df_2_array(test_df,  alpha158, target, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1865, 20, 100, 143), (465, 20, 100, 143), (466, 20, 100, 143))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_feat_train.shape, dataset_feat_eval.shape, dataset_feat_test.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
