{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1\n",
    "## Modify the raw data downloaded from `Choice` \n",
    "### In order to use `Qlib` to calculate factor `Alpha158` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "data_raw = pd.read_excel(\"../data/data_from_choice.xlsx\", index_col = 0, header = [0, 1]).T"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following code does not need to be run twice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_code, _ = zip(*list(data_raw.index))\n",
    "stock_code = list(set(stock_code))\n",
    "field_map = {\"开盘价\": \"open\", \"收盘价\": \"close\", \"最高价\": \"high\", \"最低价\": \"low\",\n",
    "             \"均价\": \"vwap\", \"涨跌幅\": \"change\", \"成交量\": \"volume\", \"成交金额\": \"money\",\n",
    "             \"复权因子（后）\": \"factor\"}\n",
    "for tmp_stock_code in tqdm(stock_code):\n",
    "    tmp_data_raw = data_raw.loc[tmp_stock_code]\n",
    "    tmp_data_raw.rename(index = field_map, inplace = True) \n",
    "    tmp_data_raw = tmp_data_raw.T\n",
    "    tmp_data_raw.insert(0, column = \"stock_code\", value = tmp_stock_code)\n",
    "    tmp_data_raw.insert(1, column = \"date\", value = tmp_data_raw.index)\n",
    "    tmp_data_raw.set_index(keys = \"stock_code\", inplace = True)\n",
    "    tmp_data_raw.to_csv(os.path.join(\"../data/data_prepared_for_qlib/\", tmp_stock_code + \".csv\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runing the system command\n",
    "\n",
    "```\n",
    "python scripts/dump_bin.py dump_all --csv_path \"D:\\文件\\研一下\\深度学习\\Quantitative Investment Based on Transformer\\data\\data_prepared_for_qlib\" --qlib_dir ~/.qlib/qlib_data/hs100_data --symbol_field_name stock_code --date_field_name date --include_fields open,high,low,close,volume,money,factor,vwap,change\n",
    "```\n",
    "\n",
    "### in the following directory\n",
    "\n",
    "```\n",
    "D:\\Download_app\\anaconda3\\Lib\\site-packages\\qlib\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2\n",
    "## Generate dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[8084:MainThread](2023-05-08 18:44:01,175) INFO - qlib.Initialization - [config.py:416] - default_conf: client.\n",
      "[8084:MainThread](2023-05-08 18:44:01,427) INFO - qlib.Initialization - [__init__.py:74] - qlib successfully initialized based on client settings.\n",
      "[8084:MainThread](2023-05-08 18:44:01,427) INFO - qlib.Initialization - [__init__.py:76] - data_path={'__DEFAULT_FREQ': WindowsPath('C:/Users/29411/.qlib/qlib_data/hs100_data')}\n",
      "[8084:MainThread](2023-05-08 18:44:39,863) INFO - qlib.timer - [log.py:128] - Time cost: 27.449s | Loading data Done\n",
      "[8084:MainThread](2023-05-08 18:44:39,991) INFO - qlib.timer - [log.py:128] - Time cost: 0.105s | DropnaLabel Done\n",
      "d:\\Download_app\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3191: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[k1] = value[k2]\n",
      "[8084:MainThread](2023-05-08 18:44:41,539) INFO - qlib.timer - [log.py:128] - Time cost: 1.547s | CSZScoreNorm Done\n",
      "[8084:MainThread](2023-05-08 18:44:41,543) INFO - qlib.timer - [log.py:128] - Time cost: 1.679s | fit & process data Done\n",
      "[8084:MainThread](2023-05-08 18:44:41,544) INFO - qlib.timer - [log.py:128] - Time cost: 29.130s | Init data Done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import qlib\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from qlib.data import D\n",
    "from qlib.contrib.data.handler import Alpha158\n",
    "\n",
    "qlib.init(provider_uri = \"~/.qlib/qlib_data/hs100_data\")\n",
    "# Set trade date and stock code\n",
    "trade_date = D.calendar(start_time = \"2016-01-04\", end_time = \"2022-12-30\", freq = \"day\")\n",
    "instruments = D.instruments(market = \"all\")\n",
    "stock_list = D.list_instruments(instruments = instruments,\n",
    "                                start_time = \"2016-01-04\",\n",
    "                                end_time = \"2022-12-30\",\n",
    "                                as_list = True)\n",
    "# Set trade date and features\n",
    "features_df = D.features(instruments = stock_list, fields = [\"$close\", \"$volume\"], start_time = \"2016-01-04\", end_time = \"2022-12-30\", freq = \"day\")\n",
    "# Calculate alpha158\n",
    "data_handler_config = {\n",
    "    \"start_time\": \"2016-01-04\",\n",
    "    \"end_time\": \"2022-12-30\",\n",
    "    \"instruments\": \"all\",\n",
    "}\n",
    "h = Alpha158(**data_handler_config)\n",
    "feature_alpha158 = h.fetch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percentage of deleted sample is: 12.29%\n"
     ]
    }
   ],
   "source": [
    "feature_alpha158 = feature_alpha158.reset_index().dropna(axis=1, how=\"all\").rename(\n",
    "    columns={\"datetime\": \"date\", \"instrument\": \"stock_code\"}\n",
    ")\n",
    "# Delete nan data\n",
    "feature_alpha158_dropna = feature_alpha158.dropna(how = \"any\")\n",
    "print(f\"The percentage of deleted sample is:{1 - len(feature_alpha158_dropna) / len(feature_alpha158): .2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000100.SZ 1469\n",
      "000301.SZ 1462\n",
      "000333.SZ 1585\n",
      "000338.SZ 1639\n",
      "000661.SZ 1622\n",
      "000708.SZ 1633\n",
      "000725.SZ 1641\n",
      "000792.SZ 1318\n",
      "000938.SZ 1602\n",
      "001979.SZ 1621\n",
      "002027.SZ 1641\n",
      "002049.SZ 1519\n",
      "002129.SZ 1228\n",
      "002241.SZ 1641\n",
      "002271.SZ 1641\n",
      "002371.SZ 1634\n",
      "002410.SZ 1641\n",
      "002415.SZ 1639\n",
      "002460.SZ 1639\n",
      "002466.SZ 1609\n",
      "002475.SZ 1640\n",
      "002493.SZ 1641\n",
      "002594.SZ 1641\n",
      "002601.SZ 1599\n",
      "002648.SZ 1639\n",
      "300014.SZ 1641\n",
      "300015.SZ 1619\n",
      "300059.SZ 1634\n",
      "300122.SZ 1641\n",
      "300124.SZ 1628\n",
      "300274.SZ 1634\n",
      "300347.SZ 1634\n",
      "300413.SZ 1373\n",
      "600010.SH 1628\n",
      "600019.SH 1543\n",
      "600028.SH 1641\n",
      "600031.SH 1641\n",
      "600036.SH 1641\n",
      "600048.SH 1632\n",
      "600050.SH 1543\n",
      "600089.SH 1633\n",
      "600111.SH 1641\n",
      "600176.SH 1627\n",
      "600219.SH 1608\n",
      "600276.SH 1640\n",
      "600309.SH 1512\n",
      "600346.SH 1526\n",
      "600406.SH 1528\n",
      "600426.SH 1641\n",
      "600436.SH 1641\n",
      "600519.SH 1641\n",
      "600570.SH 1639\n",
      "600585.SH 1641\n",
      "600588.SH 1639\n",
      "600690.SH 1639\n",
      "600893.SH 1606\n",
      "600900.SH 1628\n",
      "601012.SH 1632\n",
      "601088.SH 1575\n",
      "601225.SH 1641\n",
      "601318.SH 1641\n",
      "601390.SH 1565\n",
      "601398.SH 1641\n",
      "601600.SH 1530\n",
      "601668.SH 1640\n",
      "601766.SH 1629\n",
      "601857.SH 1641\n",
      "601888.SH 1641\n",
      "601899.SH 1633\n",
      "601919.SH 1589\n",
      "601985.SH 1641\n",
      "603799.SH 1627\n",
      "603993.SH 1620\n",
      "000063.SZ 1581\n",
      "002714.SZ 1635\n",
      "002709.SZ 1634\n",
      "600745.SH 1413\n",
      "600438.SH 1611\n",
      "002230.SZ 1602\n",
      "002352.SZ 1587\n",
      "000002.SZ 1571\n",
      "300450.SZ 1501\n",
      "002555.SZ 1522\n",
      "000651.SZ 1508\n",
      "300142.SZ 1513\n",
      "002812.SZ 1376\n",
      "603986.SH 1313\n",
      "300628.SZ 1349\n",
      "300661.SZ 1285\n",
      "603501.SH 1182\n",
      "603260.SH 1198\n",
      "603259.SH 1072\n",
      "300750.SZ 1048\n",
      "300760.SZ 964\n",
      "600989.SH 823\n",
      "300782.SZ 801\n",
      "688981.SH 538\n",
      "600905.SH 319\n",
      "601728.SH 269\n",
      "600941.SH 179\n"
     ]
    }
   ],
   "source": [
    "for stock_code in feature_alpha158_dropna['stock_code'].unique():\n",
    "    print(stock_code, len(feature_alpha158_dropna[feature_alpha158_dropna['stock_code'] == stock_code]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha158_list = feature_alpha158_dropna.columns[2:].to_list()  # alpha158\n",
    "basic_feature = [\"open\", \"close\", \"high\", \"low\", \"volume\"]\n",
    "target_return_span = 5\n",
    "target = f\"return+{target_return_span}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the raw data and features(alpha 158)\n",
    "stock_raw_data_list = []\n",
    "for file_name in os.listdir(\"../data/data_prepared_for_qlib\"):\n",
    "    tmp_data = pd.read_csv(os.path.join(\"../data/data_prepared_for_qlib\", file_name))[[\"date\", \"open\", \"close\", \"high\", \"low\", \"volume\", \"stock_code\"]]\n",
    "    tmp_data[target] = tmp_data.close.pct_change(target_return_span).shift(-1 * target_return_span) # the return of several future days \n",
    "    stock_raw_data_list.append(tmp_data)\n",
    "\n",
    "stock_raw_data_df = pd.concat(stock_raw_data_list).sort_values(by=\"date\").dropna()\n",
    "stock_raw_data_df.date = pd.DatetimeIndex(stock_raw_data_df.date)\n",
    "# The final DataFrame used for generating dataset\n",
    "stock_alpha158 = feature_alpha158_dropna.merge(stock_raw_data_df, how = \"inner\", on = [\"stock_code\", \"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the DataFrame for training, validating and testing\n",
    "train_df = stock_alpha158[(stock_alpha158.date >= \"2016-01-01\") & (stock_alpha158.date < \"2019-01-01\")]\n",
    "val_df   = stock_alpha158[(stock_alpha158.date >= \"2019-01-01\") & (stock_alpha158.date < \"2020-01-01\")]\n",
    "test_df  = stock_alpha158[(stock_alpha158.date >= \"2020-01-01\") & (stock_alpha158.date < \"2023-01-01\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\29411\\AppData\\Local\\Temp\\ipykernel_8084\\526267473.py:26: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  dataset_feat = np.array(dataset_feat).transpose((1, 2, 0, 3))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "axes don't match array",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8084\\526267473.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdataset_feat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset_ret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset_price\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m \u001b[0mdataset_feat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset_ret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset_price\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf2array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha158_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"train\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime_span\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m60\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8084\\526267473.py\u001b[0m in \u001b[0;36mdf2array\u001b[1;34m(dataset_df, feat_col, target, type, time_span)\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mdataset_price\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstock_price\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[0mdataset_feat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_feat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m     \u001b[0mdataset_ret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_ret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mdataset_price\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_price\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: axes don't match array"
     ]
    }
   ],
   "source": [
    "def df2array(dataset_df, feat_col, target, type, time_span):\n",
    "    dataset_feat = []\n",
    "    dataset_ret = []\n",
    "    dataset_price = []\n",
    "    stock_list = list(dataset_df.stock_code.unique())\n",
    "    for stock in stock_list:\n",
    "        df = dataset_df[dataset_df.stock_code == stock]\n",
    "        feat = df[feat_col].to_numpy()\n",
    "        ret = df[target].to_numpy()\n",
    "        price = df['close'].to_numpy()\n",
    "        stock_feat = []\n",
    "        stock_ret = []\n",
    "        stock_price = []\n",
    "        for i in range(time_span, feat.shape[0]):\n",
    "            stock_feat.append(feat[i-time_span : i])\n",
    "            stock_ret.append(ret[i])\n",
    "            stock_price.append(price[i])\n",
    "        stock_feat = np.array(stock_feat)\n",
    "        stock_ret = np.array(stock_ret)\n",
    "        stock_price = np.array(stock_price)\n",
    "    \n",
    "        dataset_feat.append(stock_feat)\n",
    "        dataset_ret.append(stock_ret)\n",
    "        dataset_price.append(stock_price)\n",
    "\n",
    "    dataset_feat = np.array(dataset_feat).transpose((1, 2, 0, 3))\n",
    "    dataset_ret = np.array(dataset_ret).transpose((1, 0))\n",
    "    dataset_price = np.array(dataset_price).transpose((1, 0))\n",
    "    \n",
    "    dataset_feat_tensor = torch.tensor(dataset_feat, dtype=torch.float)\n",
    "    dataset_ret_tensor = torch.tensor(dataset_ret, dtype=torch.float)\n",
    "    dataset_price_tensor = torch.tensor(dataset_price, dtype=torch.float)\n",
    "    '''\n",
    "    torch.save(dataset_feat_tensor, f\"./dataset/alpha/{type}/feat.pt\")\n",
    "    torch.save(dataset_ret_tensor, f\"./dataset/alpha/{type}/ret.pt\")\n",
    "    torch.save(dataset_price_tensor, f\"./dataset/alpha/{type}/price.pt\")    \n",
    "    '''\n",
    "    return dataset_feat, dataset_ret, dataset_price\n",
    "\n",
    "dataset_feat, dataset_ret, dataset_price = df2array(train_df, alpha158_list, target, \"train\", time_span = 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\29411\\AppData\\Local\\Temp\\ipykernel_8084\\112717159.py:26: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  dataset_feat = np.array(dataset_feat).transpose((1, 2, 0, 3))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "axes don't match array",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8084\\112717159.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mdataset_price\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstock_price\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[0mdataset_feat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_feat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[0mdataset_ret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_ret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[0mdataset_price\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_price\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: axes don't match array"
     ]
    }
   ],
   "source": [
    "dataset_df, feat_col, target, type, time_span = train_df, alpha158_list, target, \"train\", 60\n",
    "dataset_feat = []\n",
    "dataset_ret = []\n",
    "dataset_price = []\n",
    "stock_list = list(dataset_df.stock_code.unique())\n",
    "for stock in stock_list:\n",
    "    df = dataset_df[dataset_df.stock_code == stock]\n",
    "    feat = df[feat_col].to_numpy()\n",
    "    ret = df[target].to_numpy()\n",
    "    price = df['close'].to_numpy()\n",
    "    stock_feat = []\n",
    "    stock_ret = []\n",
    "    stock_price = []\n",
    "    for i in range(time_span, feat.shape[0]):\n",
    "        stock_feat.append(feat[i-time_span : i])\n",
    "        stock_ret.append(ret[i])\n",
    "        stock_price.append(price[i])\n",
    "    stock_feat = np.array(stock_feat)\n",
    "    stock_ret = np.array(stock_ret)\n",
    "    stock_price = np.array(stock_price)\n",
    "\n",
    "    dataset_feat.append(stock_feat)\n",
    "    dataset_ret.append(stock_ret)\n",
    "    dataset_price.append(stock_price)\n",
    "\n",
    "dataset_feat = np.array(dataset_feat).transpose((1, 2, 0, 3))\n",
    "dataset_ret = np.array(dataset_ret).transpose((1, 0))\n",
    "dataset_price = np.array(dataset_price).transpose((1, 0))\n",
    "\n",
    "dataset_feat_tensor = torch.tensor(dataset_feat, dtype=torch.float)\n",
    "dataset_ret_tensor = torch.tensor(dataset_ret, dtype=torch.float)\n",
    "dataset_price_tensor = torch.tensor(dataset_price, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 60, 159)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
